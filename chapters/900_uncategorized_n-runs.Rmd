# (PART) Uncategorized {.unnumbered}

# Number of Simulation Runs {#n-runs .unnumbered}

**Key Points:**

- Not hard. Do 10 runs. Report confidence intervals. Do t-tests to make comparisons. If you suspect that the results are highly variable and are not comfortable with the precision obtained from 10 runs, do more runs. (TODO: Explain what 10 runs means in terms of power for estimating effect size.)
- XX
- XX

### Overview {.unnumbered}
Traffic simulation models use randomness to manage uncertainty regarding inputs, parameters, and underlying assumptions.
By repeatedly running the model with varied values, the output range of results reflects potential effects of uncertainty in the simulation process.
Interpreting the range of results from a simulation model requires careful thinking in order to avoid the following mis-steps:

1. Avoid mis-representing uncertainty in the simulation process.
2. Avoid mis-interpreting comparisons between simulation scenarios.

These two mis-steps can be minimized with the help of statistics.
Traffic simulation can be viewed as a random sampling problem.
The simulation process produces different results for each simulation run.
While we cannot physically run the model an infinite number of times to obtain the true average, we can take a sample of several runs to make inferences about the overall process.
Statistical tools like averages, standard deviation, confidence intervals, t-tests, and others can help us interpret the sampled results.

To help answer the question *how many simulation runs are needed?* we can revisit the two mis-steps listed above.

## Avoiding \#1: Mis-representing Process Uncertainty {.unnumbered}

We need to do *enough* simulation runs so that the sample average is representative of the overall simulation process (i.e. representative of the long-term process average, or population average).
If we simply conduct one simulation run we have no way of knowing if it is representative; the next run simulation run could be very different due to the random components in the simulation model.
By collecting results from multiple runs, we can obtain a sample mean (average), standard deviation, and build a [confidence interval](#).


TODO: show a figure of the sample distribution and population average.

One formula for estimating sample size re-arranges the one-sample z-test formula for comparing a sample to an unknown (constant) population average. Solving for sample size, the formula is as follows:

$$
N \ge \left(\frac{z_{\alpha/2}*\sigma}{E}\right)^2
$$

where:

$N$ = sample size

$E$ = margin of error

$\sigma$ = standard deviation.

$z_{\alpha/2}$ = Critical z-statistic (1.96 for a 95% confidence interval)

This formula assumes that we know the simulation process standard deviation (population standard deviation). Assuming that the sample standard deviation is equal to population standard deviation may not be appropriate for small sample sizes, in which case an alternative formula using the t-statistic [TODO: show formula using t-stat](#).

### Margin of Error {.unnumbered}
The margin of error, $E$, in the sample size formula is an input by the analyst regarding how wide of a confidence interval they are willing to accept when estimating the overall simulation average and reporting results. **The value of $E$ is a judgement call.** For example:

- Some traffic engineering literature uses $E = 10%$ of the sample mean. So if the sample mean is 15, then $E = 0.10 * 15 = 1.5$ This says that the number of runs conducted should result in a 95% confidence interval within +/- 10% of the sample mean. [TODO: cite sources](#)
- Other traffic engineering literature uses field data to estimate the margin of error. [TODO: cite sources](#) However, going back to the root of avoiding mis-step #1, confidence intervals let us infer an answer to the question regarding if our sample results are representative of the simulation.
This is a **separate question** from how the simulation compares to field data (calibration).
Therefore, determining margin of error E does not need to depend on field data to avoid mis-step #1.
Assuming that variability in the field data is the same as the variability in the simulation may not be appropriate, or may lead to unreasonable sample sizes due to the following:
 
    - **TODO:** explain mismatch between field and simulation variability
    - **TODO:** explain mismatch between sources of variability (field vs sim)
    - XX


The number of simulation runs does not distinguish a crappily calibrated model from a well-calibrated model.
A tight confidence interval can be obtained from a simulation that is not-well calibrated by simply taking a large sample of runs.
Conversely, a well-calibrated simulation might have a wide confidence interval, even with a large sample, if the underlying simulation is highly variable.

## Avoiding \#2: Mis-interpreting Scenario Comparisons {.unnumbered}
Comparisons between simulation models need to consider the distrubtion of simulation outputs and not simply compare averages in order to minimize reporting differences that could be due to random chance.
Average results from two simulation scenarios may look different, but statistically may not be different due to the distribution, or uncertainty, surrounding the averages.
The meaning of *statistically different* and *practically different* is another topic. Just because two results may be statistically different does not necessarily mean that the differences are important for real-world decision making.

To comparing two independent samples (such as results from two simulation models, or comparing field data and simulation results), we can use a [t-test or (z-test)](#).
(TODO link to page demonstrating t-tests.)
While t-tests can be used to compare two samples of any size, the statistical power [TODO: explain power](#) of the test may suffer if the sample sizes are low.

We can estimate the sample size needed to maintain high statistical power for detecting practical differences. The formula for is:

$$
N = 2\left ( z_{1-\alpha/2} + z_{1-\beta} \right )^{2}\left (\frac{\sigma}{\mu_1-\mu_2}  \right )^{2}
$$

where:

$N$ = sample size needed for each scenario. Assumes equal sample sizes.

$\sigma$ = standard deviation. The pooled variance is often used, which is essentially a weighted average. Using the higher standard deviation could also be considered for a more conservative estimate of $N$.

$z_{1-\alpha/2}$ = z-value for controlling $\alpha$, typically 1.96 for 95% confidence

$z_{1-\beta}$ = z-value for controlling $\beta$, typically 0.84 for 80% power.

$\mu_1-\mu_2$ = Desired difference in means.

If we use typical assumptions of 95% confidence interval and 80% power, then the formula simplifies to the following when substituting in the corresponding z-statistics:

$$
N \ge 16 \left (\frac{\sigma}{\mu_1-\mu_2} \right )^{2}
$$



### Random thoughts
t-test is a detective, not a jury.
If you are using a t-test to decide if two simulation models are difference, then the difference between alternatives probably too small to be practical (i.e. if the simulation results are not painfully different, then the real-world differences are probably not going to be practically different)

