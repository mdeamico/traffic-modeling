# (PART) Uncategorized {.unnumbered}

# Number of Simulation Runs {#n-runs .unnumbered}

**Key Points:**

- Not hard. Do 10 runs. Report confidence intervals. Do t-tests to make comparisons. If you suspect that the results are highly variable and are not comfortable with the precision obtained from 10 runs, do more runs. (TODO: Explain what 10 runs means in terms of power for estimating effect size.)
- XX
- XX

### Overview {.unnumbered}
Traffic simulation models use randomness to manage uncertainty regarding inputs, parameters, and underlying assumptions.
By repeatedly running the model with varied values, the output range of results reflects potential effects of uncertainty in the simulation process.
Interpreting the range of results from a simulation model requires careful thinking in order to avoid the following:

1. Avoid mis-representing uncertainty in the simulation process.
2. Avoid mis-interpreting comparisons between simulation scenarios.

The above mis-steps can be minimized with the help of statistics.
Traffic simulation can be viewed as a random sampling problem.
The simulation process produces different results for each simulation run.
While we cannot physically run the model an infinite number of times, we can take a sample of several runs to make inferences about the overall process.
Statistical tools like averages, standard deviation, confidence intervals, t-tests, and others can help us interpret the sampled results.

To help answer the question *how many simulation runs are needed?* we can revisit the two mis-steps listed above.

## Avoiding \#1: Mis-representing Process Uncertainty {.unnumbered}

We need to do *enough* simulation runs so that the sample average is representative of the overall simulation process (i.e. representative of the long-term process average, or population average).
If we simply conduct one simulation run we have no way of knowing if it represents the overall simulation process; the next run simulation run could be very different due to the random components in the simulation model.
By doing multiple runs, we can obtain a sample mean (average) and standard deviation and build a confidence interval.

A [confidence interval](https://www.itl.nist.gov/div898/handbook/eda/section3/eda352.htm) is a statement about the variability in the sampling process.
(Also see [realated article](https://www.itl.nist.gov/div898/handbook/prc/section1/prc14.htm)) (Be careful if reading the [Wikipedia article](https://en.wikipedia.org/wiki/Confidence_interval) about confidence intervals, it is frequently edited and subject to controversy.) Narrower intervals indicate less variability.
For example if we take a sample of 30 simulation runs, and the mean is 29 with a 95% confidence interval of [28, 30], we could say that the sampling process has little variability when representing the overall simulation.
However, if we take a sample of 5 runs, and the mean is 27 with a 95% confidence interval of [18, 36], by comparison, we could say that the sampling process has more variability in representing the overall simulation.
If we took a different sample of 5 runs, a fairly different average and confidence interval might not be unexpected because of the higher variability.
[TODO: make an interactive confidence interval demo](#).

TODO: show a figure of the sample distribution and population average.

One formula for estimating sample size re-arranges the one-sample z-test formula for comparing a sample to an unknown (constant) population average. Solving for sample size, the formula is as follows:

$$
N \ge \left(\frac{1.96\sigma}{E}\right)^2
$$

where:

$N$ = sample size

$E$ = margin of error

$\sigma$ = standard deviation.

This formula also assumes a 95% two-sided confidence interval, and that we know the simulation process standard deviation (population standard deviation). Assuming that the sample standard deviation is equal to population standard deviation may not be appropriate for small sample sizes, in which case an alternative formula using the t-statistic [TODO: show formula using t-stat](#).

### Margin of Error {.unnumbered}
The margin of error, E, in the sample size formula is an input by the analyst regarding how wide of a confidence interval they are willing to accept when estimating the overall simulation average and reporting results. **The value of E is a judgement call.** For example:

- Some traffic engineering literature uses E = 10% of the sample mean. So if the sample mean is 15, then $E = 0.10 * 15 = 1.5$ This says that the number of runs conducted should result in a 95% confidence interval within +/- 10% of the sample mean. [TODO: cite sources](#)
- Other traffic engineering literature uses field data to estimate the margin of error. [TODO: cite sources](#) However, going back to the root of avoiding mis-step #1, confidence intervals let us infer an answer to the question regarding if our sample results are representative of the simulation. This is a **separate question** from how the simulation compares to field data (calibration). Therefore, determining margin of error E does not need to depend on field data to avoid mis-step #1. Assuming that variability in the field data is the same as the variability is a dangerous assumption **TODO, explain further...mis-match between simulation and field variability, sources of simulation variability (like car following model), sources of field variability**

The number of simulation runs does not distinguish a crappily calibrated model from a well-calibrated model.
A tight confidence interval can be obtained from a simulation that is not-well calibrated by simply taking a large sample of runs.
Conversely, a well-calibrated simulation might have a wide confidence interval, even with a large sample, if the underlying simulation is highly variable.

## Avoiding \#2: Mis-interpreting Scenario Comparisons {.unnumbered}
Comparisons between simulation models need to consider the distrubtion of simulation outputs and not simply compare averages in order to minimize reporting differences that could be due to random chance.
Average results from two simulation scenarios may look different, but statistically may not be different due to the distribution, or uncertainty, surrounding the averages.
The meaning of *statistically different* and *practically different* is another topic. Just because two results may be statistically different does not necessarily mean that the differences are important for real-world decision making.

To comparing two independent samples (such as results from two simulation models, or comparing field data and simulation results), we can use a [t-test or (z-test)](#).
(TODO link to page demonstrating t-tests.)
While t-tests can be used to compare two samples of any size, for example 10 simulation runs versus 15 field data points,
the statistical power [TODO: explain power](#) of the test may suffer if the sample sizes are low.

We can estimate the sample size needed to maintain high statistical power for detecting practical differences. The formula for is:

$$
N = 2\left ( z_{1-\alpha/2} + z_{1-\beta} \right )^{2}\left (\frac{\sigma}{\mu_1-\mu_2}  \right )^{2}
$$

where:

$N$ = sample size needed for each scenario. Assumes equal sample sizes.

$\sigma$ = standard deviation. The pooled variance is often used, which is essentially a weighted average. Using the higher standard deviation could also be considered for a more conservative estimate of $N$.

$z_{1-\alpha/2}$ = z-value for controlling $\alpha$

$z_{1-\beta}$ = z-value for controlling $\beta$

$\mu_1-\mu_2$ = Desired difference in means.

TODO show rule of thumb for alpha = 0.05 and beta = 0.80 -> 16*sigma^2/diff^2


### Random thoughts
t-test is a detective, not a jury.
