# (PART) Uncategorized {.unnumbered}

# Number of Simulation Runs {#n-runs .unnumbered}

**Traffic simulation models need to be run more than once to account for their random nature.** Using results from one model run is like rolling a pair of dice that land on double sixes, then proclaiming that 12 is the typical outcome. Averaging results from multiple rolls is necessary to arrive at the typical sum of two dice[^uncategorized_n-runs-1]. Likewise, one simulation run is probably not enough to estimate the typical simulation model output. The question is then how many simulation runs are needed? A practical approach to answering this question is as follows (assuming a typical traffic analysis process that starts by developing an existing conditions model before developing alternative scenario models):

[^uncategorized_n-runs-1]: The typical sum of two dice is 7.

1.  **Conduct 10 simulation runs as a practical starting point**. [(Why 10 runs?)](#)

    -   If the simulation is too expensive in terms of time or money to do 10 runs, then do as many runs as resources allow.

    While this discussion contains a lot of "words and math," not over-complicating the topic of number of simulation runs is encouraged. The most important part is that the analyst is doing more than one run and the results are being presented and interpreted correctly with the help of statistics. The remainder of the steps discussed below can be used as a check to understand how precise the results are for 10 runs, and can be used to estimate how many more runs may be needed if the results from 10 runs are ambiguous.

2.  **Choose a measure of effectiveness (MOE) and location of interest in the model.** The MOE and location should be related to the purpose of the model and be important for decision making. For example, if the purpose of the model is to investigate how an alternative may affect travel time along a particular section, then choosing the travel time between two critical points in the model would be appropriate. Another example could be choosing level of service at an intersection where alternative configurations are being considered.

3.  **Using the 10 simulation runs, calculate the average, standard deviation, margin of error, and confidence interval for the selected MOE.** [(Confidence Interval Formula)](#)

4.  **Inspect the obtained precision of results.** The margin of error and minimum effect size can be checked to see if the results from 10 runs have enough precision to support decision making.

    Consider an example:

    -   10 simulation runs of an existing conditions model
    -   Average travel time = 10 minutes
    -   Standard deviation = 0.75 minutes
    -   Margin of error = 0.5 minutes at the 95% confidence level.

    The margin of error suggests that the true simulation average in this example is estimated to be between 9.5 and 10.5 minutes.[^uncategorized_n-runs-2] Further, based on the standard deviation and [minimum effect size for 10 runs](#), t-tests in this example could be expected to distinguish alternatives that differ by approximately 1.0 minute or more. The analyst and decision makers must decide if this level of precision is acceptable for the project.

    Inspecting margin of error and effect size is an engineering judgement call, and is not a matter of statistics [@lisa_issues_nodate]. **There is no magic formula that can decide if the precision obtained from 10 runs is of practical importance** ([see discussion](#)). In the above example, does the true simulation average need to be estimated more precisely than $\pm$ 0.5 minutes? Would project decisions change if alternatives were different by less than 1.0 minute? If the answer to either question is 'yes', then more simulation runs should be considered to increase precision in the results. Keep in mind that more runs comes at potential increased resource costs and does not necessarily increase model quality [(see discussion regarding interpreting statistics)](#).

5.  **Do more runs if the results from 10 runs are not precise enough to support project decisions.** If the precision is too low, differences between alternatives may be indistinguishable. Statistical t-tests help avoid making erroneous conclusions. For example, consider two alternatives that have average travel times of 9.8 minutes and 10.2 minutes. Depending on the variability in the simulation, the apparent difference between the averages may be due to random chance (i.e. statistically insignificant) and concluding they are different would be a misinterpretation of the results.

    Use the [sample size formula related to effect size](#) to help estimate the required number of runs to achieve statistically significant results for comparing alternatives. The [sample size formula related to margin of error](#) can also be used to estimate the number of runs required to achieve the desired precision from one model.

    It is possible for the sample size formulas to return an impractical number of runs. In these cases, either decrease the desired confidence level and/or increase the acceptable margin of error to obtain a practical number of runs, or accept that the model may not have the precision to distinguish alternatives. Neither of these outcomes necessarily mean that the model is bad or wrong. However, the results may require more careful interpretation and communication.

[^uncategorized_n-runs-2]: The true simulation average is unknown; it could be 12 minutes, or some other other value. Repeatedly doing 10 runs and constructing a confidence interval for each repetion would be expected to capture the true average 95% of the time; there is no way to know if any one particular confidence interval actually captured the true average.

## Why 10 runs? {.unnumbered}

Ten runs has both statistical and practical merit.

**Practical guides developed over time have included 10 runs as a minimum** [@oregon_department_of_transportation_protocol_2011; @virginia_department_of_transportation_notitle_2013; @virginia_department_of_transportation_vdot_2020]. The VDOT Vissim 2020 guidelines go as far as saying *"In most cases, ten simulation runs or less will be sufficient. In instances where the [VDOT Sample Size Determination] tool recommends more than ten simulation runs, considerable debugging and evaluation of model performance is recommended."*[@virginia_department_of_transportation_vdot_2020]

Statistically, **10 runs is enough to allow statistical comparisons (t-tests) to detect an [effect size](#) of 1.32** (assuming 95% confidence and 80% power). Multiplying effect size by standard deviation provides an estimate of the smallest statistically detectable difference between results. For example, if two alternative simulations are conducted with 10 runs each and the simulated travel time results have a standard deviation of 0.75 minutes, then t-tests could detect approximately 1.0 minute or larger differences between the alternatives (1.32 \* 0.75 minutes). Smaller standard deviation or more runs allow statistical detection of smaller differences. See [discussion](#) regarding estimating samples for a given effect size.

While less common, **10 runs is also enough to check if the estimated mean is within 1 standard deviation of a constant value** (assuming 95% confidence and 80% power, [@noauthor_nist/sematech_2013])[^uncategorized_n-runs-3] (TODO add footnote with formula showing how 1 SD result is derived from effect size formula). For example, consider 10 runs that have an average travel time of 10 minutes with a standard deviation of 0.75. Further assume that it is important to know if the travel time is different from 9.25 minutes (1 standard deviation away). A t-test would have the ability to detect if the simulation average is statistically different from a constant value of 9.25 minutes assuming 95% confidence and 80% power (similarly, 10.75 minutes could be checked).

[^uncategorized_n-runs-3]: This is derived from the effect size formula for a continuous outcome, one process random sampling problem:

    $$
    N = \left ( z_{1-\alpha/2} + z_{1-\beta} \right )^{2} (\frac{1}{d})^{2}
    $$

    with the following assumptions:

    $N$ = sample size (number of runs)

    $d$ = effect size = 1.0 to estimate within 1 standard deviation

    $t_{1-\alpha/2,\ \nu = N-1 = 9}$ = 2.26 for 5% significance (95% confidence interval)

    $t_{1-\beta,\ \nu = N-1 = 9}$ = 0.88 for 80% power.

If the results from 10 runs are not precise enough, refining the number of runs requires two things: (1) having an estimate of the model's standard deviation, which can be obtained from the initial set of 10 runs, and (2) having a clear objective of the simulation, which is typically to compare two or more alternatives. The sufficiency of the number of runs for statistically comparing alternatives can be checked by considering [effect size](#).

The next sections discuss formulas for estimating the number of runs to achieve desired statistical properties.

## Background on the Statistics {.unnumbered}

Traffic simulation models use random variables to mimic distributions of real-world phenomena such as vehicle departure times, traffic compositions (mix of passenger cars & trucks), driver behaviors, and others. By repeatedly running the model with different values for the random variables, the outputs reflect a range of results. Interpreting the range of results requires careful thinking in order to avoid the following issues:

1.  Mis-representing uncertainty in the simulation process.
2.  Mis-interpreting comparisons between simulation scenarios.

These two issues can be minimized with the help of statistics. Statistical tools like averages, standard deviation, confidence intervals, t-tests, and others can help interpret the sampled results.

To help answer the question *how many simulation runs are needed,* the two issues listed above are explored in the next sections.

## Avoiding Issue #1: Mis-representing Process Uncertainty {.unnumbered}

Suppose you ask a friend to roll some dice and then tell you the sum (you don't know how many dice were rolled). Would the results from one roll give you a lot of confidence about estimating the typical sum of the unknown number of dice? Traffic simulation is the same way due to the randomness in the model. Issue #1 is about make sure that the analyst recognizes the process uncertainty by doing more than one run, thus a lot of traffic engineering literature focuses on this issue. Especially given that typical projects start with building an existing conditions model from scratch where the model variability is unknown.

Doing an infinite number of runs to obtain the true simulation average is not practical. Instead, a sample of several model runs can allow inferences about the overall process. The sample average, standard deviation, and a [confidence interval](#) can be constructed help to avoid issue #1. Repeatedly running one model is analogous to a "continuous outcome, one process" random sampling problem with the goal of achieving an informative confidence interval [@lisa_issues_nodate]. For example, if only 3 simulation runs are performed, and the resulting confidence interval for a simulated delay is 30 seconds $\pm$ 25 seconds, this suggests that there is anywhere between essentially no delay (5 seconds) and 55 seconds of delay. More than 3 runs could be helpful in this circumstance to get a better understanding of the variability in the simulation.

### Formula {.unnumbered}

Estimating *how many runs is enough* to address issue #1 can be accomplished by re-arranging the [confidence interval](#) formula because Solving for sample size, the formula is as follows:

$$
N \ge \left(\frac{z_{\alpha/2}*\sigma}{E}\right)^2
$$

where:

$N$ = sample size

$E$ = [margin of error]

$\sigma$ = standard deviation

$z_{\alpha/2}$ = Critical z-statistic (1.96 for a 95% confidence interval)

This formula provides a practical way to estimate the number of runs required to represent the overall simulation process, but it does come with assumptions:

-   Using the z-statistic makes the math easier, but is less accurate for small sample sizes $(N < 30)$.
-   The sample standard deviation ($s$) obtained from an initial set of runs is assumed to represent the population standard deviation ($\sigma$).

Using the z-statistic is close enough for practical purposes because the formula is simply an estimate based on an initial trial number of runs. Conducting more runs may be needed if the initial number of trial runs was smaller than estimated by the formula. If using the t-statistic instead of the z-statistic, solving this formula requires an iterative solution.

-   When reporting confidence intervals for final results, or conducting statistical comparisons, the t-statistic should be used to be conservative, especially for small samples $(N < 30)$.

### Margin of Error {.unnumbered}

The margin of error, $E$, in the sample size formula is an input by the analyst regarding how wide of a confidence interval they are willing to accept when estimating the overall simulation average and reporting results. **The value of** $E$ **is a judgement call**. The analyst must decide how wide of a confidence interval is too wide to be useful given the context of the problem they are trying to solve and the constraints they are working within. Their decision is not a matter of statistics, but rather a matter of practical application [@lisa_issues_nodate]. Here are some examples regarding choosing $E$:

-   Some traffic engineering literature uses $E = sample\ mean * 10\%$ [@oregon_department_of_transportation_protocol_2011; @virginia_department_of_transportation_notitle_2013] or 5% of the mean as a practical minimum [@noauthor_traffic_2019]. For example, if using the 10% guide and the sample mean is 15, then $E = 15 * 0.10 = 1.5$. While this provides a starting point, the relative percentages may or may not provide a reasonable margin or error, or realistic number of runs, depending on the units of the performance measure and inherent variability in the simulation. Inspecting the absolute range, not just the relative range, of the margin of error is encouraged when judging acceptable precision.

-   Other traffic engineering literature uses field data to estimate the margin of error [@noauthor_guidance_2014]. However, going back to the root of avoiding issue #1, the goal of the confidence interval is to answer to the question *are the sample results representative of the overall simulation process?* This question is a *separate question* from how the simulation compares to field data (calibration), which is more related to estimating sample sizes for [comparing scenarios](#). For the purposes of avoiding issue #1, choosing an acceptable margin of error E is entirely concerned with the simulation process, not field data. Using field variability to estimate simulation sample size may not be appropriate, or may lead to unreasonable sample sizes. Especially if there is a mismatch between the field and simulated sources, or magnitude, of variability.

    -   For example, the field data may come from a relatively homogeneous group of observations with little variability. However, there may be underlying assumptions or behaviors in the model that are out of the users control that lead to higher variability in the simulation outputs. Expecting low variability in the simulation results could lead to a high number of runs. Attempting to control the simulation variability may create an unrealistic expectation that the simulation should act deterministically and could tempt the analyst to overconstrain model parameters. Neither attempting to control the variability, nor conducting a high number of runs is efficient from a practical perspective.
    -   The literature recognizes that using field data could lead to a high number of runs and suggests 5% of the mean as a minimum margin of error [@noauthor_traffic_2019]. If using a 5% minimum based on judgement, then why not just use 5% as a conservative value instead of making assumptions based on field data? If 5% requires too many runs to be practical, then try 10% or other compromise, or do the minimum practical number of runs that the analyst feels can still produce a reasonable estimate (confidence interval) for the simulation average. (If considering the number of runs required for comparing scenarios, see discussion on [issue #2](#).)

The number of simulation runs does not distinguish a crappily calibrated model from a well-calibrated model. A tight confidence interval can be obtained from a simulation that is not well-calibrated by simply taking a large sample of runs. Conversely, a high-quality, well-calibrated simulation might have a wide confidence interval, even with a large sample, if the underlying simulation is highly variable.

### Examples {.unnumbered}

**TODO include examples of how to use the formula**

## Avoiding Issue #2: Mis-interpreting Scenario Comparisons {.unnumbered}

Comparisons between simulation models need to consider the distribution of simulation outputs in order to avoid reporting differences that could be due to random chance. Average results from two simulation scenarios may look different, but statistically may not be different due to the distribution, or uncertainty, surrounding the averages. The meaning of *statistically different* and *practically different* is another topic. Just because two results may be statistically different does not necessarily mean that the differences are important for real-world decision making.

To compare two independent samples, such as results from two simulation models, a [t-test or (z-test)](#) can be used. (TODO link to page demonstrating t-tests.) While t-tests can be used to compare two samples of any size, the [statistical power](#) of the test may suffer if the sample sizes are low, meaning that there is a high chance of concluding that there is no difference between alternatives when there could be (i.e. arriving at a false negative conclusion).

The sample size needed to maintain good statistical power can be estimated using this simplified formula that uses the typical assumptions of 95% confidence interval and 80% power:

$$
N \ge 16 \left (\frac{1}{d} \right )^{2}
$$

Where:

$N$ = sample size needed for each scenario. Assumes equal sample sizes.

$d$ = effect size. This is a relative metric calculated as $d = \frac{\mu_1-\mu_2}{\sigma}$. In other words, effect size is the minimum desired difference between means ($\mu$) normalized by standard deviation ($\sigma$). **Like [margin of error], the choice of effect size is an engineering judgement call.** For example, is it of practical importance if average travel between alternatives are different by 1 minute? Would project decisions change if differences were less than 1 minute? These are questions cannot be answered by statistics. In the example, effect size $d$ would be calculated by dividing 1.0 minute by an estimate of the standard deviation discussed next.

Statistics literature discusses several effect size estimates, including: Cohen's d, Hedges' g, Glass's $\Delta$, among others. All of these differ in the way that they normalize the difference between means (i.e. differing estimates of standard deviation, $\sigma$). Here is a suggestion:

-   Use the pooled standard deviation ($\sigma_p$) if the ratio between the variances is approximately between 0.5 and 2.0. This range is a rule-of-thumb for when standard deviations are "close enough" to be pooled [\@pennstate_dept_of_stats_7.3.1.1_nodate].
-   If not using the pooled standard deviation, try using the larger $\sigma$ of the two alternatives to be conservative. Or use if only one scenario has been conducted so far, use its $\sigma$ for now and compare with the other scenario $\sigma$ later.
-   Try a few different estimates of $\sigma$ to see how sensitive $N$ is. You may need an iterative trial-and-error approach to find a reasonable balance between sample size and effect size.

### Examples {.unnumbered}

**TODO include examples of how to use the formula**

Tables

**TODO** show example tables of n-runs vs effect size

#### Full Formula {.unnumbered}

The simplified formula above assumes 95% confidence and 80% power. If different assumptions are needed, the full formula is:

$$
N = 2\left ( z_{1-\alpha/2} + z_{1-\beta} \right )^{2} (\frac{1}{d})^{2}
$$

where variables are defined above, and additionally:

$z_{1-\alpha/2}$ = z-value for controlling $\alpha$, typically 1.96 for 5% significance

$z_{1-\beta}$ = z-value for controlling $\beta$, typically 0.84 for 80% power.

Using the z-statistic is close enough for practical purposes because the formula is simply an estimate based on an initial trial number of runs. Conducting more runs may be needed if the initial number of trial runs was smaller than estimated by the formula. If using the t-statistic instead of the z-statistic, solving this formula requires an iterative solution.

-   When doing hypothesis tests to compare scenarios for final results, the t-statistic should be used to be conservative, especially for small samples $(N < 30)$.

## **A few notes about statistics** {#stat-notes .unnumbered}

-   **Statistics are useful to avoid concluding that two results are different when they may not be.** For example, if one model has an average travel time of 10.0 minutes, and an alternative model has an average of 9.7 minutes. The difference between results could be due to random chance if the model variability is high enough and/or the number of runs is too low to detect a difference of 0.3 minutes. If a t-test comes back statistically insignificant, the analyst should either conclude that there may not be a difference between alternatives, or conduct additional runs if detecting the potentially small difference between alternatives is important.

-   **Statistically significant does not necessarily mean practically significant.** Statistical t-tests can be easily gamed to achieve significance by simply increasing the sample size (number of runs). The t-test results (p-values) are a detective, not a jury (TODO: CITE NOVA), meaning that the test results point to something that may be interesting but do not have the final say in making decisions. The analyst/project team makes decisions, not the model or math. Differences between alternatives always need to be viewed from a practical perspective to decide if they are important or meaningful. Also, statistically significant results do not necessarily mean that the model is high quality. Model assumptions and limitations also need to be taken into consideration when interpreting results. p-value is a detective, not a jury. <https://www.pbs.org/video/what-p-value-wmmwgl/> NOVA Season 45 Episode 6 Prediction by the Numbers Jordan Ellenberg (UW Madison)

-   Acknowledge things beyond t-tests, ANOVA, etc

-   

<!-- -->

-   Few runs or low statistical confidence does make a model bad or wrong; however, the results require more careful interpretation.

## Random thoughts {.unnumbered}

If you are using a t-test to decide if two simulation models are difference, then the difference between alternatives probably too small to be practical (i.e. if the simulation results are not painfully different, then the real-world differences are probably not going to be practically different)

2004 TAT III appendix talks about sample size estimation for confidence intervals and t-tests, seems similar to the "2 issues" idea.

old thoughts on issue #1:

How to estimate the number of runs required to avoid issue #1:

1.  Choose a measure of effectiveness (MOE) and location of interest in the model. The MOE and location should be related to the purpose of the model. For example, if the purpose of the model is to investigate how an alternative may affect travel time along a particular section, then choosing the travel time between two critical points in the model would be appropriate. Another example could be choosing level of service at an intersection where alternative configurations are being considered.
2.  Conduct an initial set of runs. Doing 10 runs is suggested, if this amount is practical in terms of computation time. Obtain the average and standard deviation for the MOE of interest.
3.  Check E from the initial set of runs. Decide if E is acceptable. Acceptability is a practical issue, not a statistics issue.

...

1.  Use the [formula] discussed below to estimate how many runs would be required to obtain a user-specified [margin of error].

2.  If the [formula] suggests more simulation runs are needed, the actual obtained margin of error $E$ from the initial set of runs should inspected as it may be close enough for practical purposes. (TODO: provide an example/link to an example.) Adjusting expectations regarding margin of error and/or confidence level may be necessary in order to maintain a practical number of runs. A wide margin of error or lower confidence level does not mean that the model is somehow bad or useless, it just means that the final results may not be as precise as originally intended. Using statistics to compare results in these cases can still help avoid erroneous conclusions. A power analysis should also be conducted to see if the initial number of runs allows detecting a reasonable effect size. (TODO: hyperlink). If a wider margin of error, lower confidence, or large minimum effect size cannot be accepted, then conduct the additional runs as indicated by the formulas.

3.  Report performance measure results based on the final number of runs. Use the t-statistic when reporting confidence intervals or doing paired tests.

`r if (knitr::is_html_output()) '## References {.unnumbered}'`
